{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63de4277-c079-1630-2e47-44d138f3e7bf"
   },
   "source": [
    "# Library Import\n",
    "The goal of this file is to predict the opening price for DJIA with the help of CNN and LSTM individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "193a91a4-7404-a01b-2c78-6b7385979b9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f98318d0-c122-24a1-a9a6-62ab93985428"
   },
   "source": [
    "# Data Import\n",
    "We will divide the data into traning and testing \n",
    "- Traiing dat < 2014\n",
    "- Test data >= 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "28217d5a-e580-fa92-fdc2-45a4842812e0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed_data/Combined_News_DJIA.csv')\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d372c26-3c4c-7890-3089-c8ddb17ca81a"
   },
   "source": [
    "# Data Process\n",
    "- Transform the news headlines into number of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a87f0d3d-80f3-af90-9368-dedde8c1154f"
   },
   "outputs": [],
   "source": [
    "# headlines for training\n",
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headlines for testing\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "66913298-12f3-da89-030a-90ba38b1e11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 31675)\n"
     ]
    }
   ],
   "source": [
    "# vectorizing train headlines\n",
    "basicvectorizer = CountVectorizer()\n",
    "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
    "print(basictrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "682e897f-169c-8694-d05d-66e9fc844158"
   },
   "source": [
    "## Deep Learning\n",
    "- tf-idf vectorizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ac32628f-8080-5d53-e3a2-3d6986431f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 401)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label the data for target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1611, 401)\n",
      "X_test shape: (378, 401)\n"
     ]
    }
   ],
   "source": [
    "X_train = advancedtrain.toarray()\n",
    "X_test = advancedtest.toarray()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(train[\"Label\"])\n",
    "y_test = np.array(test[\"Label\"])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [],
   "source": [
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(X_train)\n",
    "X_train /= scale\n",
    "X_test /= scale\n",
    "\n",
    "mean = np.mean(X_train)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Here\\'s a Deep Dumb MLP (DDMLP)\\nmodel = Sequential()\\nmodel.add(Dense(256, input_dim=input_dim))\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(Dropout(0.4))\\nmodel.add(Dense(128))\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(Dropout(0.4))\\nmodel.add(Dense(nb_classes))\\nmodel.add(Activation(\\'softmax\\'))\\n\\n# we\\'ll use categorical xent for the loss, and RMSprop as the optimizer\\nmodel.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'rmsprop\\')\\n\\nprint(\"Training...\")\\nmodel.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15)#, show_accuracy=True)\\n\\nprint(\"Generating test predictions...\")\\npreds14 = model.predict_classes(X_test, verbose=0)\\nacc14 = accuracy_score(test[\"Label\"], preds14)\\n\\nprint(\\'prediction accuracy: \\', acc14)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15)#, show_accuracy=True)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds14 = model.predict_classes(X_test, verbose=0)\n",
    "acc14 = accuracy_score(test[\"Label\"], preds14)\n",
    "\n",
    "print('prediction accuracy: ', acc14)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "143c5d6d-4259-c945-f1dd-b07a4c388526"
   },
   "source": [
    "### Applying first model LSTM \n",
    "- Following are the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "db232b70-f3a0-0952-e7ab-9c0b768e8025"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "e5f6e70a-325d-5a94-0c16-c852f951d475"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(trainheadlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "a9224d6a-3bca-ef84-9817-a72b41241f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (1611, 200)\n",
      "X_test shape: (378, 200)\n"
     ]
    }
   ],
   "source": [
    "# padding the sequences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- Build the model\n",
    "- Training of model\n",
    "- Evaluate the model accuracy\n",
    "- Generating test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "29359f0d-8d42-2038-c04f-2cd5bc87bc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/3\n",
      "1611/1611 [==============================] - 64s 40ms/step - loss: 0.6919 - acc: 0.5425 - val_loss: 0.6935 - val_acc: 0.5079\n",
      "Epoch 2/3\n",
      "1611/1611 [==============================] - 64s 40ms/step - loss: 0.6208 - acc: 0.6760 - val_loss: 0.6932 - val_acc: 0.5741\n",
      "Epoch 3/3\n",
      "1611/1611 [==============================] - 62s 38ms/step - loss: 0.3211 - acc: 0.9081 - val_loss: 0.9814 - val_acc: 0.5476\n",
      "378/378 [==============================] - 5s 13ms/step\n",
      "Test score: 0.981368827441382\n",
      "Test accuracy: 0.5476190477767319\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds_lstm = model.predict_classes(X_test, verbose=0)\n",
    "acc_lstm = accuracy_score(test['Label'], preds_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.5476190476190477\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Precision Recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5474056459971952, 0.5476190476190477, 0.5458146791480124, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(test['Label'], preds_lstm, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5458146791480124"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test['Label'], preds_lstm, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8afb0d07-7d0d-dd18-b19c-be603f301b52"
   },
   "source": [
    "## Applying second model CNN\n",
    "- Hyperparameters are following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "7271a1d1-cde0-a26c-de99-edfb7dfbf918"
   },
   "outputs": [],
   "source": [
    "nb_filter = 120\n",
    "filter_length = 2\n",
    "hidden_dims = 120\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- Build the model\n",
    "- Training of model\n",
    "- Evaluate the model accuracy\n",
    "- Generating test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=120, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "61e07990-43ef-ff27-3b36-dca146f8b4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/1\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.6922 - acc: 0.5292 - val_loss: 0.6962 - val_acc: 0.5079\n",
      "378/378 [==============================] - 0s 949us/step\n",
      "Test score: 0.6962450723799448\n",
      "Test accuracy: 0.5079365098287189\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds_cnn = model.predict_classes(X_test, verbose=0)\n",
    "acc_cnn = accuracy_score(test['Label'], preds_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Precision Recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "49e9abbe-d224-b757-c17b-bd4cfa5da818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.5079365079365079\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25799949609473416, 0.5079365079365079, 0.3421888053467001, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(test['Label'], preds_cnn, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
